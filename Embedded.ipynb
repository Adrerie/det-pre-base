{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3256be9",
   "metadata": {},
   "source": [
    "此处用于聚合所有模块进行训练\n",
    "首先两两组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_with_eca_csab.py\n",
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "from eca_module import ECA\n",
    "from csab_offset import CSAB_Offset\n",
    "\n",
    "# === Step 1: 加载我们已有的最优基线模型 ===\n",
    "# 确保所有实验都从同一起点开始，保证公平性。\n",
    "print(\"Loading baseline model: yolo11n_final4.pt\")\n",
    "model = YOLO(\"yolo11n_final4.pt\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# === Step 2: 定义一个函数来同时注入两个模块 ===\n",
    "def inject_eca_and_csab(model, eca_layers=(4, 7, 10), csab_layers=(8, 10), eca_k_size=3):\n",
    "    \"\"\"\n",
    "    一个函数内完成ECA和CSAB_Offset模块的注入。\n",
    "    - ECA 模块将被包裹在原始层外部。\n",
    "    - CSAB_Offset 模块也将以类似方式注入，增强空间特征提取。\n",
    "    \"\"\"\n",
    "    print(\"Starting module injection process...\")\n",
    "    \n",
    "    # 注入 ECA 模块\n",
    "    for i in eca_layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            # 获取输出通道数\n",
    "            if hasattr(old_layer, 'cv2'):\n",
    "                ch = old_layer.cv2.conv.out_channels\n",
    "            elif isinstance(old_layer, nn.Sequential) and hasattr(list(old_layer.children())[-1], 'out_channels'):\n",
    "                 ch = list(old_layer.children())[-1].out_channels\n",
    "            else:\n",
    "                 # 尝试从常见的属性获取通道数\n",
    "                 ch = old_layer.out_channels if hasattr(old_layer, 'out_channels') else old_layer.conv.out_channels\n",
    "            \n",
    "            # 使用nn.Sequential将原始层和ECA模块包裹起来\n",
    "            model.model[i] = nn.Sequential(old_layer, ECA(ch, eca_k_size))\n",
    "            print(f\"✅ Injected ECA after layer {i}, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject ECA at layer {i}: {e}\")\n",
    "\n",
    "    # 注入 CSAB_Offset 模块\n",
    "    # 注意：这里的注入逻辑需要根据您的模型结构精确调整\n",
    "    # 一个常见的做法是替换掉某个关键的 block 或者 Conv 层\n",
    "    for i in csab_layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            # 同样地，获取通道数\n",
    "            if hasattr(old_layer, 'cv2'):\n",
    "                 ch = old_layer.cv2.conv.out_channels\n",
    "            else:\n",
    "                 ch = old_layer.out_channels if hasattr(old_layer, 'out_channels') else old_layer.conv.out_channels\n",
    "            \n",
    "            # 这里我们用CSAB_Offset替换原始层\n",
    "            model.model[i] = CSAB_Offset(ch) \n",
    "            print(f\"✅ Replaced layer {i} with CSAB_Offset, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject CSAB at layer {i}: {e}\")\n",
    "\n",
    "    print(\"Module injection complete.\")\n",
    "    return model\n",
    "\n",
    "# === Step 3: 注入模块并验证模型结构 ===\n",
    "# 我们选择在骨干网络深层的关键block后插入模块，这些地方的特征图语义信息更丰富\n",
    "model = inject_eca_and_csab(model, eca_layers=[6, 9], csab_layers=[10, 13])\n",
    "\n",
    "# === Step 4: 保存注入了双模块的结构版模型 ===\n",
    "# 这个权重是随机初始化的（对于新模块），需要Finetune\n",
    "init_weights_path = \"weights/yolov11n_eca_csab_init.pt\"\n",
    "model.save(init_weights_path)\n",
    "print(f\"Initial model with ECA and CSAB modules saved to {init_weights_path}\")\n",
    "\n",
    "# === Step 5: 在ECA+CSAB版本上进行Finetune训练 ===\n",
    "# 使用与单模块实验一致的超参数以进行公平比较\n",
    "print(\"Starting Finetune training for ECA + CSAB model...\")\n",
    "results = model.train(\n",
    "    data=\"pest24.yaml\",\n",
    "    seed=42,\n",
    "    epochs=50,\n",
    "    batch=24,\n",
    "    workers=2,\n",
    "    lr0=1e-3,                \n",
    "    lrf=0.05,                 \n",
    "    optimizer=\"AdamW\",\n",
    "    weight_decay=0.001,\n",
    "    dropout=0.2,\n",
    "    cos_lr=True,              # 使用余弦退火学习率\n",
    "    cache=True,\n",
    "    amp=True,\n",
    "    device=0,\n",
    "    patience=15,              # 耐心值\n",
    "    save_period=10,\n",
    "    name='finetune_eca_csab'   # 清晰的实验命名\n",
    ")\n",
    "\n",
    "# === Step 6: 保存最终Finetune完成的权重 ===\n",
    "final_weights_path = \"weights/yolov11n_eca_csab_finetuned.pt\"\n",
    "model.save(final_weights_path)\n",
    "print(f\"Final finetuned model with ECA and CSAB saved to {final_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83051716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_with_eca_dbl.py\n",
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "from eca_module import ECA\n",
    "from dbl_loss_tsd import DBL_TSD\n",
    "\n",
    "# === Step 1: 加载我们已有的最优基线模型 ===\n",
    "print(\"Loading baseline model: yolo11n_final4.pt\")\n",
    "model = YOLO(\"yolo11n_final4.pt\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# === Step 2: 注入 ECA 模块 ===\n",
    "# 沿用之前的注入逻辑\n",
    "def inject_eca(model, layers=(4, 7, 10), k_size=3):\n",
    "    print(\"Starting ECA module injection...\")\n",
    "    for i in layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            # 智能获取输出通道数\n",
    "            ch = -1\n",
    "            if hasattr(old_layer, 'cv2') and hasattr(old_layer.cv2, 'conv'):\n",
    "                ch = old_layer.cv2.conv.out_channels\n",
    "            elif isinstance(old_layer, nn.Sequential) and hasattr(list(old_layer.children())[-1], 'out_channels'):\n",
    "                 ch = list(old_layer.children())[-1].out_channels\n",
    "            else: # 备用方案\n",
    "                 ch = old_layer.out_channels if hasattr(old_layer, 'out_channels') else old_layer.conv.out_channels\n",
    "            \n",
    "            model.model[i] = nn.Sequential(old_layer, ECA(ch, k_size))\n",
    "            print(f\"✅ Injected ECA after layer {i}, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject ECA at layer {i}: {e}\")\n",
    "    print(\"ECA injection complete.\")\n",
    "    return model\n",
    "\n",
    "model = inject_eca(model, layers=[6, 9]) # 在骨干网深层注入\n",
    "\n",
    "# === Step 3: 定义并注册 DBL_TSD 损失函数回调 ===\n",
    "print(\"Setting up DBL_TSD loss callback...\")\n",
    "loss_wrapper = DBL_TSD(cls_w=1.0, box_w=5.0, iou_w=2.0, scale_alpha=1.0, trunc_thresh=0.4)\n",
    "\n",
    "def custom_loss_callback(trainer):\n",
    "    # 注意: trainer对象的属性可能因版本而异，这里基于您的代码结构\n",
    "    # 您可能需要调试并确认'preds'和'targets'的正确获取方式\n",
    "    try:\n",
    "        preds = trainer.preds # Ultralytics v8+ 通常在这里\n",
    "        targets = trainer.batch\n",
    "        loss_val, info = loss_wrapper(preds, targets)\n",
    "        trainer.loss = loss_val\n",
    "    except Exception as e:\n",
    "        # 如果上述方式失败，这是一个备用方案，但可能不完全准确\n",
    "        # print(f\"Callback Warning: Could not apply custom loss via standard attributes. Error: {e}\")\n",
    "        pass # 保持默认损失计算\n",
    "    return trainer.loss\n",
    "\n",
    "model.add_callback(\"on_train_batch_end\", custom_loss_callback)\n",
    "print(\"✅ DBL_TSD loss callback registered.\")\n",
    "\n",
    "\n",
    "# === Step 4: 保存注入了双模块的结构版模型 ===\n",
    "init_weights_path = \"weights/yolov11n_eca_dbl_init.pt\"\n",
    "model.save(init_weights_path)\n",
    "print(f\"Initial model with ECA and DBL_TSD setup saved to {init_weights_path}\")\n",
    "\n",
    "# === Step 5: 在 ECA+DBL_TSD 版本上进行Finetune训练 ===\n",
    "print(\"Starting Finetune training for ECA + DBL_TSD model...\")\n",
    "results = model.train(\n",
    "    data=\"pest24.yaml\",\n",
    "    seed=42,\n",
    "    epochs=50,\n",
    "    batch=24,\n",
    "    workers=2,\n",
    "    lr0=1e-3,                \n",
    "    lrf=0.05,                 \n",
    "    optimizer=\"AdamW\",\n",
    "    weight_decay=0.001,\n",
    "    dropout=0.2,\n",
    "    cos_lr=True,\n",
    "    cache=True,\n",
    "    amp=True,\n",
    "    device=0,\n",
    "    patience=15,\n",
    "    save_period=10,\n",
    "    name='finetune_eca_dbl'   # 清晰的实验命名\n",
    ")\n",
    "\n",
    "# === Step 6: 保存最终Finetune完成的权重 ===\n",
    "final_weights_path = \"weights/yolov11n_eca_dbl_finetuned.pt\"\n",
    "model.save(final_weights_path)\n",
    "print(f\"Final finetuned model with ECA and DBL_TSD saved to {final_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_with_csab_dbl.py\n",
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "from csab_offset import CSAB_Offset\n",
    "from dbl_loss_tsd import DBL_TSD\n",
    "\n",
    "# === Step 1: 加载我们已有的最优基线模型 ===\n",
    "print(\"Loading baseline model: yolo11n_final4.pt\")\n",
    "model = YOLO(\"yolo11n_final4.pt\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# === Step 2: 注入 CSAB_Offset 模块 ===\n",
    "def inject_csab_offset(model, layers=(10, 13)):\n",
    "    print(\"Starting CSAB_Offset module injection...\")\n",
    "    for i in layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            # 获取输出通道数\n",
    "            ch = -1\n",
    "            if hasattr(old_layer, 'cv2') and hasattr(old_layer.cv2, 'conv'):\n",
    "                 ch = old_layer.cv2.conv.out_channels\n",
    "            else:\n",
    "                 ch = old_layer.out_channels if hasattr(old_layer, 'out_channels') else old_layer.conv.out_channels\n",
    "            \n",
    "            # 使用CSAB_Offset模块替换原有的层\n",
    "            model.model[i] = CSAB_Offset(ch) \n",
    "            print(f\"✅ Replaced layer {i} with CSAB_Offset, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject CSAB at layer {i}: {e}\")\n",
    "    print(\"CSAB_Offset injection complete.\")\n",
    "    return model\n",
    "\n",
    "model = inject_csab_offset(model, layers=[10, 13])\n",
    "\n",
    "# === Step 3: 定义并注册 DBL_TSD 损失函数回调 ===\n",
    "print(\"Setting up DBL_TSD loss callback...\")\n",
    "loss_wrapper = DBL_TSD(cls_w=1.0, box_w=5.0, iou_w=2.0, scale_alpha=1.0, trunc_thresh=0.4)\n",
    "\n",
    "def custom_loss_callback(trainer):\n",
    "    try:\n",
    "        preds = trainer.preds\n",
    "        targets = trainer.batch\n",
    "        loss_val, info = loss_wrapper(preds, targets)\n",
    "        trainer.loss = loss_val\n",
    "    except Exception as e:\n",
    "        # print(f\"Callback Warning: Could not apply custom loss. Error: {e}\")\n",
    "        pass\n",
    "    return trainer.loss\n",
    "\n",
    "model.add_callback(\"on_train_batch_end\", custom_loss_callback)\n",
    "print(\"✅ DBL_TSD loss callback registered.\")\n",
    "\n",
    "# === Step 4: 保存注入了双模块的结构版模型 ===\n",
    "init_weights_path = \"weights/yolov11n_csab_dbl_init.pt\"\n",
    "model.save(init_weights_path)\n",
    "print(f\"Initial model with CSAB_Offset and DBL_TSD setup saved to {init_weights_path}\")\n",
    "\n",
    "# === Step 5: 在 CSAB+DBL_TSD 版本上进行Finetune训练 ===\n",
    "print(\"Starting Finetune training for CSAB_Offset + DBL_TSD model...\")\n",
    "results = model.train(\n",
    "    data=\"pest24.yaml\",\n",
    "    seed=42,\n",
    "    epochs=50,\n",
    "    batch=24,\n",
    "    workers=2,\n",
    "    lr0=1e-3,                \n",
    "    lrf=0.05,                 \n",
    "    optimizer=\"AdamW\",\n",
    "    weight_decay=0.001,\n",
    "    dropout=0.2,\n",
    "    cos_lr=True,\n",
    "    cache=True,\n",
    "    amp=True,\n",
    "    device=0,\n",
    "    patience=15,\n",
    "    save_period=10,\n",
    "    name='finetune_csab_dbl'   # 清晰的实验命名\n",
    ")\n",
    "\n",
    "# === Step 6: 保存最终Finetune完成的权重 ===\n",
    "final_weights_path = \"weights/yolov11n_csab_dbl_finetuned.pt\"\n",
    "model.save(final_weights_path)\n",
    "print(f\"Final finetuned model with CSAB_Offset and DBL_TSD saved to {final_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f40757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_with_all_modules.py\n",
    "from ultralytics import YOLO\n",
    "import torch.nn as nn\n",
    "from eca_module import ECA\n",
    "from csab_offset import CSAB_Offset\n",
    "from dbl_loss_tsd import DBL_TSD\n",
    "\n",
    "# === Step 1: 加载我们已有的最优基线模型 ===\n",
    "print(\"Loading baseline model: yolo11n_final4.pt\")\n",
    "model = YOLO(\"yolo11n_final4.pt\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# === Step 2: 定义一个函数来同时注入所有模型架构模块 ===\n",
    "def inject_all_modules(model, eca_layers=(6, 9), csab_layers=(10, 13)):\n",
    "    \"\"\"在一个函数内完成ECA和CSAB_Offset模块的注入。\"\"\"\n",
    "    print(\"Starting module injection process for ECA and CSAB_Offset...\")\n",
    "    \n",
    "    # 注入 ECA 模块\n",
    "    for i in eca_layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            ch = old_layer.cv2.conv.out_channels if hasattr(old_layer, 'cv2') else old_layer.conv.out_channels\n",
    "            model.model[i] = nn.Sequential(old_layer, ECA(ch))\n",
    "            print(f\"✅ Injected ECA after layer {i}, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject ECA at layer {i}: {e}\")\n",
    "\n",
    "    # 注入 CSAB_Offset 模块\n",
    "    for i in csab_layers:\n",
    "        try:\n",
    "            old_layer = model.model[i]\n",
    "            ch = old_layer.cv2.conv.out_channels if hasattr(old_layer, 'cv2') else old_layer.conv.out_channels\n",
    "            model.model[i] = CSAB_Offset(ch) \n",
    "            print(f\"✅ Replaced layer {i} with CSAB_Offset, channels={ch}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to inject CSAB at layer {i}: {e}\")\n",
    "\n",
    "    print(\"Model architecture module injection complete.\")\n",
    "    return model\n",
    "\n",
    "# === Step 3: 注入模型架构模块 ===\n",
    "model = inject_all_modules(model, eca_layers=[6, 9], csab_layers=[10, 13])\n",
    "\n",
    "# === Step 4: 定义并注册 DBL_TSD 损失函数回调 ===\n",
    "print(\"Setting up DBL_TSD loss callback...\")\n",
    "loss_wrapper = DBL_TSD(cls_w=1.0, box_w=5.0, iou_w=2.0, scale_alpha=1.0, trunc_thresh=0.4)\n",
    "\n",
    "def custom_loss_callback(trainer):\n",
    "    try:\n",
    "        preds = trainer.preds\n",
    "        targets = trainer.batch\n",
    "        loss_val, info = loss_wrapper(preds, targets)\n",
    "        trainer.loss = loss_val\n",
    "    except Exception as e:\n",
    "        # print(f\"Callback Warning: Could not apply custom loss. Error: {e}\")\n",
    "        pass\n",
    "    return trainer.loss\n",
    "\n",
    "model.add_callback(\"on_train_batch_end\", custom_loss_callback)\n",
    "print(\"✅ DBL_TSD loss callback registered.\")\n",
    "\n",
    "\n",
    "# === Step 5: 保存注入了所有模块的结构版模型 ===\n",
    "init_weights_path = \"weights/yolov11n_eca_csab_dbl_init.pt\"\n",
    "model.save(init_weights_path)\n",
    "print(f\"Initial model with all modules setup saved to {init_weights_path}\")\n",
    "\n",
    "# === Step 6: 在“三合一”版本上进行Finetune训练 ===\n",
    "print(\"Starting Finetune training for the ultimate ECA + CSAB_Offset + DBL_TSD model...\")\n",
    "results = model.train(\n",
    "    data=\"pest24.yaml\",\n",
    "    seed=42,\n",
    "    epochs=50,\n",
    "    batch=24,\n",
    "    workers=2,\n",
    "    lr0=1e-3,                \n",
    "    lrf=0.05,                 \n",
    "    optimizer=\"AdamW\",\n",
    "    weight_decay=0.001,\n",
    "    dropout=0.2,\n",
    "    cos_lr=True,\n",
    "    cache=True,\n",
    "    amp=True,\n",
    "    device=0,\n",
    "    patience=15,\n",
    "    save_period=10,\n",
    "    name='finetune_eca_csab_dbl'   # 清晰的实验命名\n",
    ")\n",
    "\n",
    "# === Step 7: 保存最终Finetune完成的权重 ===\n",
    "final_weights_path = \"weights/yolov11n_eca_csab_dbl_finetuned.pt\"\n",
    "model.save(final_weights_path)\n",
    "print(f\"Final finetuned model with all modules saved to {final_weights_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
